# Docker Compose for local development and testing
# 
# Usage:
#   docker-compose up -d          # Start all services
#   docker-compose up api         # Start API only (local inference)
#   docker-compose up api worker  # Start API with GPU worker
#
# For GPU support, ensure nvidia-docker is installed and configured.

version: "3.8"

services:
  # ==========================================================================
  # API Service (CPU-only)
  # ==========================================================================
  api:
    build:
      context: ..
      dockerfile: deploy/api/Dockerfile
    ports:
      - "8000:8000"
    environment:
      - ENVIRONMENT=development
      - INFERENCE_MODE=local  # Change to "remote" to use GPU worker
      - WORKER_URLS=http://worker:8080
      - ENABLE_DOCS=true
      - ENABLE_METRICS=true
      - LOG_LEVEL=INFO
      - LOG_FORMAT=console
      - ENCRYPTION_KEY=dev-encryption-key-not-for-production
      - LIKELIHOOD_PARAMS_PATH=/app/data/likelihood_params.json
      - MASK_PATH=/app/data/mask.npy
    volumes:
      - ../service_data:/app/service_data
      - artifacts:/app/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/v1/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    depends_on:
      worker:
        condition: service_healthy
        required: false
    networks:
      - watermarking
  
  # ==========================================================================
  # GPU Worker Service
  # ==========================================================================
  worker:
    build:
      context: ..
      dockerfile: deploy/worker/Dockerfile
    ports:
      - "8080:8080"
    environment:
      - MODEL_ID=runwayml/stable-diffusion-v1-5
      - DEVICE=cuda  # Use "cpu" or "mps" if no GPU
      - USE_FP16=true
      - MAX_CONCURRENT_REQUESTS=4
      - MAX_QUEUE_SIZE=16
      - GPU_SEMAPHORE_SIZE=2
      - ENABLE_WARMUP=true
      - ENABLE_DOCS=true
      - LOG_LEVEL=INFO
      - LOG_FORMAT=console
      - LIKELIHOOD_PARAMS_PATH=/app/data/likelihood_params.json
      - MASK_PATH=/app/data/mask.npy
    volumes:
      - huggingface_cache:/home/worker/.cache/huggingface
      - artifacts:/app/data
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/v1/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 300s  # 5 minutes for model loading
    networks:
      - watermarking
  
  # ==========================================================================
  # PostgreSQL (optional, for production-like setup)
  # ==========================================================================
  postgres:
    image: postgres:15-alpine
    ports:
      - "5432:5432"
    environment:
      - POSTGRES_USER=watermarking
      - POSTGRES_PASSWORD=watermarking_dev
      - POSTGRES_DB=watermarking
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U watermarking"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - watermarking
    profiles:
      - database
  
  # ==========================================================================
  # Redis (for distributed rate limiting)
  # ==========================================================================
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - watermarking
    profiles:
      - cache

volumes:
  huggingface_cache:
    name: watermarking_hf_cache
  postgres_data:
    name: watermarking_postgres
  artifacts:
    name: watermarking_artifacts

networks:
  watermarking:
    name: watermarking_network

