{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Mode Watermarking - Quick Start\n",
        "\n",
        "This notebook demonstrates the complete workflow for watermarking images with Stable Diffusion, including:\n",
        "- Configuration setup\n",
        "- Dataset generation (watermarked/unwatermarked)\n",
        "- Statistical detection\n",
        "- Detector training\n",
        "- Evaluation and metrics\n",
        "\n",
        "## Overview\n",
        "\n",
        "The mode-watermarking system embeds secret watermarks into images generated by Stable Diffusion using non-distortionary or distortionary modes. This notebook provides an interactive tutorial for using the system.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1: Setup & Installation\n",
        "\n",
        "First, we'll set up the environment and import necessary modules.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add parent directory to path for imports\n",
        "import sys\n",
        "from pathlib import Path\n",
        "sys.path.insert(0, str(Path('.').resolve().parent))\n",
        "\n",
        "# Standard imports\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Dict, Any, Optional\n",
        "\n",
        "# Check device availability\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Import mode-watermarking modules\n",
        "from src.config.config_loader import ConfigLoader\n",
        "from src.sd_integration.sd_client import SDClient\n",
        "from src.detection.recovery import recover_g_values\n",
        "from src.detection.correlate import compute_s_statistic, batch_compute_s_statistics\n",
        "from src.detection.calibrate import calibrate_thresholds\n",
        "from src.evaluation.quality_metrics import compute_quality_metrics, batch_compute_quality_metrics\n",
        "from src.evaluation.eval import run_full_evaluation\n",
        "from src.evaluation.visualize import plot_roc_curve, plot_score_distributions\n",
        "\n",
        "print(\"Imports successful!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2: Configuration Overview\n",
        "\n",
        "Load and examine the configuration files that control watermark embedding, diffusion parameters, and model architecture.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize config loader\n",
        "config_loader = ConfigLoader()\n",
        "\n",
        "# Load configurations\n",
        "watermark_cfg = config_loader.load_watermark_config(\"configs/watermark_config.yaml\")\n",
        "diffusion_cfg = config_loader.load_diffusion_config(\"configs/diffusion_config.yaml\")\n",
        "model_cfg = config_loader.load_model_architecture_config(\"configs/model_architecture.yaml\")\n",
        "\n",
        "# Display key parameters\n",
        "print(\"=\" * 60)\n",
        "print(\"Configuration Overview\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\nWatermark Configuration:\")\n",
        "print(f\"  Mode: {watermark_cfg.get('bias', {}).get('mode', 'N/A')}\")\n",
        "print(f\"  Key Scheme: {watermark_cfg.get('watermark', {}).get('key_scheme', 'N/A')}\")\n",
        "print(f\"  G-field Shape: {watermark_cfg.get('watermark', {}).get('g_field', {}).get('shape', 'N/A')}\")\n",
        "print(f\"  Mapping Mode: {watermark_cfg.get('watermark', {}).get('g_field', {}).get('mapping_mode', 'N/A')}\")\n",
        "\n",
        "print(\"\\nDiffusion Configuration:\")\n",
        "print(f\"  Trained Timesteps: {diffusion_cfg.get('diffusion', {}).get('trained_timesteps', 'N/A')}\")\n",
        "print(f\"  Inference Timesteps: {diffusion_cfg.get('diffusion', {}).get('inference_timesteps', 'N/A')}\")\n",
        "print(f\"  Scheduler: {diffusion_cfg.get('diffusion', {}).get('scheduler', 'N/A')}\")\n",
        "\n",
        "print(\"\\nModel Configuration:\")\n",
        "print(f\"  Model ID: {model_cfg.get('model_id', 'N/A')}\")\n",
        "print(f\"  Use FP16: {model_cfg.get('use_fp16', 'N/A')}\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3: Initialize SD Client\n",
        "\n",
        "Set up the Stable Diffusion pipeline with watermark embedding support.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare config paths\n",
        "config_paths = {\n",
        "    \"diffusion\": \"configs/diffusion_config.yaml\",\n",
        "    \"watermark\": \"configs/watermark_config.yaml\",\n",
        "    \"model\": \"configs/model_architecture.yaml\"\n",
        "}\n",
        "\n",
        "# Initialize SD client\n",
        "print(\"Initializing Stable Diffusion pipeline...\")\n",
        "sd_client = SDClient(config_paths=config_paths, device=device)\n",
        "sd_client.initialize_pipeline()\n",
        "\n",
        "print(\"Pipeline initialized successfully!\")\n",
        "print(f\"Pipeline device: {sd_client.device}\")\n",
        "\n",
        "# Register watermark embedding hook (this is done automatically during generate)\n",
        "# The hook will be registered when we generate watermarked images\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4: Dataset Generation\n",
        "\n",
        "Generate watermarked and unwatermarked images from prompts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate single watermarked image\n",
        "prompt = \"A beautiful sunset over mountains with vibrant colors\"\n",
        "\n",
        "print(f\"Generating watermarked image with prompt: '{prompt}'\")\n",
        "image_wm, manifest_wm = sd_client.generate(\n",
        "    prompt=prompt,\n",
        "    seed=42,  # For reproducibility\n",
        "    num_inference_steps=None,  # Use config default\n",
        "    guidance_scale=None  # Use config default\n",
        ")\n",
        "\n",
        "print(f\"\\nGenerated image size: {image_wm.size}\")\n",
        "print(f\"Manifest keys: {list(manifest_wm.keys())}\")\n",
        "print(f\"Sample ID: {manifest_wm.get('sample_id', 'N/A')}\")\n",
        "print(f\"Mode: {manifest_wm.get('mode', 'N/A')}\")\n",
        "\n",
        "# Display image\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.imshow(image_wm)\n",
        "plt.axis('off')\n",
        "plt.title(f\"Watermarked Image\\nPrompt: {prompt[:50]}...\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate batch of images\n",
        "prompts = [\n",
        "    \"A serene lake at dawn\",\n",
        "    \"A futuristic cityscape at night\",\n",
        "    \"A field of wildflowers in spring\"\n",
        "]\n",
        "\n",
        "print(f\"Generating {len(prompts)} watermarked images...\")\n",
        "images_wm = []\n",
        "manifests_wm = []\n",
        "\n",
        "for i, prompt in enumerate(prompts):\n",
        "    print(f\"  [{i+1}/{len(prompts)}] {prompt}\")\n",
        "    image, manifest = sd_client.generate(\n",
        "        prompt=prompt,\n",
        "        seed=42 + i,\n",
        "        num_inference_steps=None,\n",
        "        guidance_scale=None\n",
        "    )\n",
        "    images_wm.append(image)\n",
        "    manifests_wm.append(manifest)\n",
        "\n",
        "print(\"\\nBatch generation complete!\")\n",
        "\n",
        "# Display batch\n",
        "fig, axes = plt.subplots(1, len(images_wm), figsize=(15, 5))\n",
        "for i, (img, prompt) in enumerate(zip(images_wm, prompts)):\n",
        "    axes[i].imshow(img)\n",
        "    axes[i].axis('off')\n",
        "    axes[i].set_title(prompt[:30] + \"...\", fontsize=10)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 5: Statistical Detection\n",
        "\n",
        "Recover g-values from watermarked images and compute detection statistics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Recover g-values from watermarked image\n",
        "print(\"Recovering g-values from watermarked image...\")\n",
        "\n",
        "recovery_result = recover_g_values(\n",
        "    image=image_wm,\n",
        "    vae_encoder=sd_client._pipeline.vae,\n",
        "    watermark_cfg=watermark_cfg,\n",
        "    key_info=manifest_wm.get(\"key_info\", {}),\n",
        "    device=device\n",
        ")\n",
        "\n",
        "print(f\"Recovered g-values shape: {recovery_result['g_values'].shape}\")\n",
        "print(f\"Latent shape: {recovery_result['latent'].shape}\")\n",
        "print(f\"Recovery metadata: {recovery_result['recovery_metadata']}\")\n",
        "\n",
        "# Visualize g-values (spatial distribution)\n",
        "g_values_mean = recovery_result['g_values'].mean(axis=0)  # Average across timesteps\n",
        "g_values_spatial = np.abs(g_values_mean).mean(axis=0)  # Average across channels, abs\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(g_values_spatial, cmap='hot')\n",
        "plt.colorbar(label='G-value Magnitude')\n",
        "plt.title('Spatial Distribution of Recovered G-values')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(g_values_mean.flatten(), bins=50, edgecolor='black', alpha=0.7)\n",
        "plt.xlabel('G-value')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('G-value Distribution')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute S-statistic (correlation between recovered and expected g-values)\n",
        "# For demonstration, we'll use the recovered g-values as both recovered and expected\n",
        "# In practice, expected g-values would be reconstructed from key_info\n",
        "\n",
        "from src.watermark.gfield import GFieldBuilder\n",
        "from src.watermark.key import KeyDerivation\n",
        "from src.sd_integration.timestep_mapper import TimestepMapper\n",
        "\n",
        "# Reconstruct expected g-values from key_info\n",
        "key_derivation = KeyDerivation()\n",
        "key_info = manifest_wm.get(\"key_info\", {})\n",
        "experiment_id = manifest_wm.get(\"experiment_id\", \"exp_001\")\n",
        "sample_id = manifest_wm.get(\"sample_id\", \"unknown\")\n",
        "base_seed = manifest_wm.get(\"sample_seed\", 42)\n",
        "zT_hash = manifest_wm.get(\"zT_hash\", \"default_hash\")\n",
        "\n",
        "# Derive seed\n",
        "key_master = key_info.get(\"key_master\", watermark_cfg.get(\"watermark\", {}).get(\"key_master\", \"\"))\n",
        "seed0 = key_derivation.derive_seed(\n",
        "    key_master=key_master,\n",
        "    sample_id=sample_id,\n",
        "    zT_hash=zT_hash,\n",
        "    base_seed=base_seed,\n",
        "    experiment_id=experiment_id\n",
        ")\n",
        "\n",
        "# Build expected g-field\n",
        "gfield_builder = GFieldBuilder(\n",
        "    mapping_mode=key_info.get(\"mapping_mode\", \"binary\"),\n",
        "    bit_pos=key_info.get(\"bit_pos\", 30)\n",
        ")\n",
        "\n",
        "timestep_mapper = TimestepMapper(\n",
        "    trained_timesteps=diffusion_cfg[\"diffusion\"][\"trained_timesteps\"],\n",
        "    inference_timesteps=diffusion_cfg[\"diffusion\"][\"inference_timesteps\"]\n",
        ")\n",
        "\n",
        "latent_shape = tuple(watermark_cfg[\"watermark\"][\"g_field\"][\"shape\"])\n",
        "stream_len = latent_shape[0] * latent_shape[1] * latent_shape[2] * len(timestep_mapper.get_all_trained_timesteps())\n",
        "key_stream = key_derivation.generate_key_stream(seed0, stream_len)\n",
        "g_schedule = gfield_builder.build_g_schedule(timestep_mapper, latent_shape, key_stream)\n",
        "\n",
        "# Use first timestep's g-field as expected\n",
        "expected_g = list(g_schedule.values())[0]\n",
        "\n",
        "# Compute S-statistic\n",
        "recovered_g_single = recovery_result['g_values'][0]  # First timestep\n",
        "s_score = compute_s_statistic(\n",
        "    recovered_g_values=recovered_g_single,\n",
        "    expected_g_values=expected_g,\n",
        "    mask=recovery_result['mask'],\n",
        "    method=\"correlation\"\n",
        ")\n",
        "\n",
        "print(f\"S-statistic (correlation): {s_score:.4f}\")\n",
        "print(f\"Interpretation: {'Watermarked' if s_score > 0.5 else 'Unwatermarked'} (threshold: 0.5)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 6: Detector Training (Optional)\n",
        "\n",
        "Train a UNet detector for watermark detection. This is a demo that trains for a few epochs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Note: Full training requires a dataset with train/val splits\n",
        "# This is a demonstration of how to set up training\n",
        "\n",
        "print(\"Detector Training Setup (Demo)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "from src.training.train import train_unet_detector\n",
        "from pathlib import Path\n",
        "\n",
        "# Check if training data exists\n",
        "train_manifest = \"data/splits/train.json\"\n",
        "val_manifest = \"data/splits/val.json\"\n",
        "\n",
        "if Path(train_manifest).exists() and Path(val_manifest).exists():\n",
        "    print(f\"Training data found:\")\n",
        "    print(f\"  Train: {train_manifest}\")\n",
        "    print(f\"  Val: {val_manifest}\")\n",
        "    print(\"\\nTo train, uncomment the code below:\")\n",
        "    print(\"\"\"\n",
        "    config_paths = {\n",
        "        \"train\": \"configs/train_config.yaml\"\n",
        "    }\n",
        "    \n",
        "    results = train_unet_detector(\n",
        "        config_paths=config_paths,\n",
        "        sd_pipeline=sd_client._pipeline,\n",
        "        resume_from_checkpoint=None\n",
        "    )\n",
        "    \n",
        "    print(f\"Best checkpoint: {results['best_checkpoint']}\")\n",
        "    print(f\"Final metrics: {results['final_metrics']}\")\n",
        "    \"\"\")\n",
        "else:\n",
        "    print(\"Training data not found.\")\n",
        "    print(f\"Expected: {train_manifest}\")\n",
        "    print(f\"Expected: {val_manifest}\")\n",
        "    print(\"\\nGenerate dataset first using:\")\n",
        "    print(\"  python scripts/generate_dataset.py --mode both --prompts-file data/coco/prompts_train.txt\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 7: Evaluation\n",
        "\n",
        "Run detection and quality metrics evaluation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quality metrics evaluation\n",
        "print(\"Computing Quality Metrics\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# For demonstration, compare watermarked image with itself (in practice, compare with original)\n",
        "# In real evaluation, you'd load original images from manifest\n",
        "\n",
        "quality_metrics = compute_quality_metrics(\n",
        "    watermarked_image=image_wm,\n",
        "    original_image=image_wm,  # In practice, use original unwatermarked image\n",
        "    metrics=[\"psnr\", \"ssim\", \"lpips\"],\n",
        "    device=device\n",
        ")\n",
        "\n",
        "print(\"\\nQuality Metrics:\")\n",
        "for metric_name, metric_value in quality_metrics.items():\n",
        "    if not np.isnan(metric_value):\n",
        "        print(f\"  {metric_name.upper()}: {metric_value:.4f}\")\n",
        "    else:\n",
        "        print(f\"  {metric_name.upper()}: N/A (computation failed or requires batch)\")\n",
        "\n",
        "print(\"\\nNote: For full evaluation with detection metrics and ROC curves,\")\n",
        "print(\"use run_full_evaluation() with a test manifest.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Full evaluation (if test manifest exists)\n",
        "test_manifest = \"data/splits/test.json\"\n",
        "\n",
        "if Path(test_manifest).exists():\n",
        "    print(\"Running full evaluation pipeline...\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    try:\n",
        "        eval_results = run_full_evaluation(\n",
        "            test_manifest=test_manifest,\n",
        "            eval_config_path=\"configs/eval_config.yaml\",\n",
        "            watermark_cfg_path=\"configs/watermark_config.yaml\",\n",
        "            diffusion_cfg_path=\"configs/diffusion_config.yaml\",\n",
        "            model_arch_cfg_path=\"configs/model_architecture.yaml\",\n",
        "            sd_pipeline=sd_client._pipeline,\n",
        "            output_dir=\"outputs/evaluation\"\n",
        "        )\n",
        "        \n",
        "        print(\"\\nEvaluation Results Summary:\")\n",
        "        print(f\"  Output directory: {eval_results.get('output_dir', 'N/A')}\")\n",
        "        \n",
        "        if eval_results.get(\"detection\"):\n",
        "            detection = eval_results[\"detection\"]\n",
        "            metrics = detection.get(\"metrics\", {})\n",
        "            print(f\"\\nDetection Metrics:\")\n",
        "            print(f\"  Accuracy: {metrics.get('accuracy', 'N/A'):.4f}\" if isinstance(metrics.get('accuracy'), float) else f\"  Accuracy: N/A\")\n",
        "            print(f\"  Precision: {metrics.get('precision', 'N/A'):.4f}\" if isinstance(metrics.get('precision'), float) else f\"  Precision: N/A\")\n",
        "            print(f\"  Recall: {metrics.get('recall', 'N/A'):.4f}\" if isinstance(metrics.get('recall'), float) else f\"  Recall: N/A\")\n",
        "            \n",
        "        if eval_results.get(\"quality\"):\n",
        "            quality = eval_results[\"quality\"]\n",
        "            overall = quality.get(\"overall\", {})\n",
        "            print(f\"\\nQuality Metrics (Overall):\")\n",
        "            for metric_name, metric_data in overall.items():\n",
        "                if isinstance(metric_data, dict) and \"mean\" in metric_data:\n",
        "                    print(f\"  {metric_name.upper()}: {metric_data['mean']:.4f} ± {metric_data.get('std', 0):.4f}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Evaluation failed: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "else:\n",
        "    print(f\"Test manifest not found: {test_manifest}\")\n",
        "    print(\"\\nTo run evaluation:\")\n",
        "    print(\"  1. Generate dataset with train/val/test splits\")\n",
        "    print(\"  2. Train a detector model\")\n",
        "    print(\"  3. Run: python -m src.cli.eval --test-manifest data/splits/test.json\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 8: Advanced Examples\n",
        "\n",
        "Advanced use cases and customization options.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Custom alpha schedule\n",
        "print(\"Custom Alpha Schedule Example\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "from src.watermark.g_utils import generate_adaptive_schedule, generate_fixed_schedule\n",
        "\n",
        "# Generate different alpha schedules\n",
        "adaptive_schedule = generate_adaptive_schedule(\n",
        "    num_timesteps=50,\n",
        "    strength_range=(0.01, 0.03),\n",
        "    peak_timestep=0.4,\n",
        "    injection_start=0.8,\n",
        "    injection_end=0.2\n",
        ")\n",
        "\n",
        "fixed_schedule = generate_fixed_schedule([0.0, 0.01, 0.02, 0.02, 0.01, 0.0])\n",
        "\n",
        "print(f\"Adaptive schedule length: {len(adaptive_schedule)}\")\n",
        "print(f\"  Min: {min(adaptive_schedule):.4f}, Max: {max(adaptive_schedule):.4f}\")\n",
        "print(f\"Fixed schedule: {fixed_schedule}\")\n",
        "\n",
        "# Plot schedules\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(adaptive_schedule, label='Adaptive Schedule', linewidth=2)\n",
        "plt.xlabel('Timestep')\n",
        "plt.ylabel('Alpha (Watermark Strength)')\n",
        "plt.title('Alpha Schedule Comparison')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nYou can customize alpha schedules in watermark_config.yaml\")\n",
        "print(\"to control watermark strength at different denoising steps.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Verify alpha maximum range for target SNR\n",
        "print(\"Alpha Maximum Verification\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "from src.watermark.g_utils import verify_alpha_max\n",
        "\n",
        "result = verify_alpha_max(\n",
        "    mode=\"non_distortionary\",\n",
        "    target_snr=0.005,  # 0.5% relative to latent noise\n",
        "    g_field_shape=(4, 64, 64),\n",
        "    latent_shape=(4, 64, 64),\n",
        "    beta_start=diffusion_cfg[\"diffusion\"][\"noise_schedule\"][\"beta_start\"],\n",
        "    beta_end=diffusion_cfg[\"diffusion\"][\"noise_schedule\"][\"beta_end\"],\n",
        "    num_timesteps=diffusion_cfg[\"diffusion\"][\"trained_timesteps\"],\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "print(f\"\\nVerification Results:\")\n",
        "print(f\"  Alpha max: {result['alpha_max']:.6f}\")\n",
        "print(f\"  Latent noise energy: {result['latent_noise_energy']:.2f}\")\n",
        "print(f\"  G-field energy: {result['g_field_energy']:.2f}\")\n",
        "print(f\"  Is valid: {result['is_valid']}\")\n",
        "print(f\"  Acceptable range: {result['acceptable_range']}\")\n",
        "\n",
        "print(\"\\nThis helps ensure watermark strength is appropriate for target SNR.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrated:\n",
        "1. ✅ Configuration loading and inspection\n",
        "2. ✅ SD pipeline initialization with watermark support\n",
        "3. ✅ Single and batch image generation\n",
        "4. ✅ G-value recovery from watermarked images\n",
        "5. ✅ Statistical detection (S-statistic computation)\n",
        "6. ✅ Quality metrics evaluation\n",
        "7. ✅ Advanced customization (alpha schedules, SNR verification)\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- **Generate Full Dataset**: Use `scripts/generate_dataset.py` to create train/val/test splits\n",
        "- **Train Detector**: Use `scripts/run_train.sh` to train UNet or Bayesian detector\n",
        "- **Run Evaluation**: Use `scripts/run_eval.sh` for comprehensive evaluation\n",
        "- **Full Pipeline**: Use `scripts/run_full_pipeline.sh` for end-to-end workflow\n",
        "\n",
        "### Resources\n",
        "\n",
        "- Config files: `configs/`\n",
        "- Documentation: See README.md\n",
        "- Scripts: `scripts/`\n",
        "- Source code: `src/`\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
