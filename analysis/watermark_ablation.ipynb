{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Watermark Ablation Analysis\n",
        "\n",
        "This notebook analyzes results from the watermark ablation experiment.\n",
        "\n",
        "## Objectives\n",
        "1. Load all result JSONs\n",
        "2. Compute ROC curves, AUC, TPR@FPR=0.01\n",
        "3. Compute summary statistics\n",
        "4. Create visualizations\n",
        "5. Rank configurations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
        "\n",
        "# Set style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams[\"figure.figsize\"] = (12, 8)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load All Result JSONs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use project root for path resolution (robust to notebook execution context)\n",
        "# Try to find project root: if we're in analysis/, go up one level; otherwise assume we're in project root\n",
        "current_dir = Path.cwd()\n",
        "if current_dir.name == \"analysis\":\n",
        "    project_root = current_dir.parent\n",
        "elif (current_dir / \"experiments\").exists():\n",
        "    project_root = current_dir\n",
        "else:\n",
        "    # Fallback: try parent directory\n",
        "    project_root = current_dir.parent\n",
        "\n",
        "results_dir = project_root / \"experiments\" / \"watermark_ablation\" / \"results\"\n",
        "\n",
        "# Final fallback to relative path\n",
        "if not results_dir.exists():\n",
        "    results_dir = Path(\"../experiments/watermark_ablation/results\")\n",
        "\n",
        "# Find all result JSON files\n",
        "result_files = sorted(results_dir.glob(\"*.json\"))\n",
        "print(f\"Found {len(result_files)} result files\")\n",
        "\n",
        "if len(result_files) == 0:\n",
        "    print(\"‚ö†Ô∏è  No result files found. Please run the experiment first:\")\n",
        "    print(\"   python scripts/run_watermark_ablation.py --prompts-file <path> --master-key <key>\")\n",
        "else:\n",
        "    # Load all results\n",
        "    results = {}\n",
        "    for result_file in result_files:\n",
        "        config_name = result_file.stem\n",
        "        try:\n",
        "            with open(result_file, \"r\") as f:\n",
        "                results[config_name] = json.load(f)\n",
        "            print(f\"  ‚úì Loaded: {config_name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚úó Failed to load {config_name}: {e}\")\n",
        "    \n",
        "    print(f\"\\n‚úì Successfully loaded {len(results)} result files\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Compute Metrics for Each Config\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_roc_curve(scores: np.ndarray, labels: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\"Compute ROC curve.\"\"\"\n",
        "    fpr, tpr, thresholds = roc_curve(labels, scores)\n",
        "    return fpr, tpr, thresholds\n",
        "\n",
        "\n",
        "def find_tpr_at_fpr(fpr: np.ndarray, tpr: np.ndarray, target_fpr: float = 0.01) -> float:\n",
        "    \"\"\"Find TPR at target FPR.\"\"\"\n",
        "    # Find index where FPR is closest to target\n",
        "    idx = np.argmin(np.abs(fpr - target_fpr))\n",
        "    return float(tpr[idx])\n",
        "\n",
        "\n",
        "def compute_predicted_separation(n_eff: float, delta_hat: float) -> float:\n",
        "    \"\"\"\n",
        "    Compute heuristic predicted separation (approximate): ŒîŒº ‚âà 4 * N_eff * Œ¥_hat^2\n",
        "    \n",
        "    NOTE: This is only a heuristic approximation and should not be treated as a primary metric.\n",
        "    \n",
        "    Args:\n",
        "        n_eff: Mean effective number of positions\n",
        "        delta_hat: Mean(p_hat - 0.5) for watermarked samples\n",
        "        \n",
        "    Returns:\n",
        "        Heuristic predicted separation (approximate)\n",
        "    \"\"\"\n",
        "    if n_eff <= 0 or abs(delta_hat) < 1e-10:\n",
        "        return 0.0\n",
        "    return 4.0 * n_eff * (delta_hat ** 2)\n",
        "\n",
        "\n",
        "# Compute metrics for each config\n",
        "summary_data = []\n",
        "\n",
        "for config_name, data in results.items():\n",
        "    # Extract separated arrays from new schema\n",
        "    log_odds_wm = np.array(data[\"log_odds_wm\"])\n",
        "    log_odds_clean = np.array(data[\"log_odds_clean\"])\n",
        "    n_eff_wm = np.array(data[\"N_eff_wm\"])\n",
        "    n_eff_clean = np.array(data[\"N_eff_clean\"])\n",
        "    p_hat_wm = np.array(data[\"p_hat_wm\"])\n",
        "    p_hat_clean = np.array(data[\"p_hat_clean\"])\n",
        "    \n",
        "    # üî• Fix 3: ROC data integrity assertions\n",
        "    # Assert array lengths match before concatenation to prevent silent corruption\n",
        "    assert len(log_odds_wm) == len(n_eff_wm), (\n",
        "        f\"Length mismatch for {config_name}: \"\n",
        "        f\"log_odds_wm ({len(log_odds_wm)}) != N_eff_wm ({len(n_eff_wm)})\"\n",
        "    )\n",
        "    assert len(log_odds_wm) == len(p_hat_wm), (\n",
        "        f\"Length mismatch for {config_name}: \"\n",
        "        f\"log_odds_wm ({len(log_odds_wm)}) != p_hat_wm ({len(p_hat_wm)})\"\n",
        "    )\n",
        "    assert len(log_odds_clean) == len(n_eff_clean), (\n",
        "        f\"Length mismatch for {config_name}: \"\n",
        "        f\"log_odds_clean ({len(log_odds_clean)}) != N_eff_clean ({len(n_eff_clean)})\"\n",
        "    )\n",
        "    assert len(log_odds_clean) == len(p_hat_clean), (\n",
        "        f\"Length mismatch for {config_name}: \"\n",
        "        f\"log_odds_clean ({len(log_odds_clean)}) != p_hat_clean ({len(p_hat_clean)})\"\n",
        "    )\n",
        "    \n",
        "    # Combine scores and labels for ROC\n",
        "    scores = np.concatenate([log_odds_wm, log_odds_clean])\n",
        "    labels = np.concatenate([np.ones(len(log_odds_wm)), np.zeros(len(log_odds_clean))])\n",
        "    \n",
        "    # Compute ROC curve\n",
        "    fpr, tpr, thresholds = compute_roc_curve(scores, labels)\n",
        "    \n",
        "    # Compute AUC\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    \n",
        "    # Compute TPR @ FPR = 0.01\n",
        "    tpr_at_1pct = find_tpr_at_fpr(fpr, tpr, target_fpr=0.01)\n",
        "    \n",
        "    # Compute statistics using separated wm and clean arrays\n",
        "    mean_log_odds_wm = float(np.mean(log_odds_wm))\n",
        "    std_log_odds_wm = float(np.std(log_odds_wm))\n",
        "    mean_log_odds_clean = float(np.mean(log_odds_clean))\n",
        "    std_log_odds_clean = float(np.std(log_odds_clean))\n",
        "    \n",
        "    # Separate N_eff statistics\n",
        "    mean_n_eff_wm = float(np.mean(n_eff_wm)) if len(n_eff_wm) > 0 else 0.0\n",
        "    mean_n_eff_clean = float(np.mean(n_eff_clean)) if len(n_eff_clean) > 0 else 0.0\n",
        "    \n",
        "    # Separate p_hat statistics\n",
        "    mean_p_hat_wm = float(np.mean(p_hat_wm)) if len(p_hat_wm) > 0 else 0.0\n",
        "    mean_p_hat_clean = float(np.mean(p_hat_clean)) if len(p_hat_clean) > 0 else 0.0\n",
        "    \n",
        "    # Compute delta_hat = mean(p_hat_wm - 0.5) for watermarked samples only\n",
        "    delta_hat = float(np.mean(p_hat_wm - 0.5)) if len(p_hat_wm) > 0 else 0.0\n",
        "    \n",
        "    # Compute heuristic predicted separation (approximate - for exploratory visualization only)\n",
        "    heuristic_predicted_separation = compute_predicted_separation(mean_n_eff_wm, delta_hat)\n",
        "    \n",
        "    # Compute empirical separation (mean log_odds difference) - primary metric\n",
        "    empirical_separation = mean_log_odds_wm - mean_log_odds_clean\n",
        "    \n",
        "    # Extract metadata fields (if available)\n",
        "    metadata_fields = {\n",
        "        \"config_path\": data.get(\"config_path\"),\n",
        "        \"num_samples\": data.get(\"num_samples\"),\n",
        "        \"num_inversion_steps\": data.get(\"num_inversion_steps\"),\n",
        "        \"likelihood_model_path\": data.get(\"likelihood_model_path\"),\n",
        "        \"timestamp\": data.get(\"timestamp\"),\n",
        "        \"git_commit\": data.get(\"git_commit\"),\n",
        "    }\n",
        "    \n",
        "    summary_data.append({\n",
        "        \"config_name\": config_name,\n",
        "        \"auc\": roc_auc,\n",
        "        \"tpr_at_1pct_fpr\": tpr_at_1pct,\n",
        "        \"mean_log_odds_wm\": mean_log_odds_wm,\n",
        "        \"std_log_odds_wm\": std_log_odds_wm,\n",
        "        \"mean_log_odds_clean\": mean_log_odds_clean,\n",
        "        \"std_log_odds_clean\": std_log_odds_clean,\n",
        "        \"mean_n_eff_wm\": mean_n_eff_wm,\n",
        "        \"mean_n_eff_clean\": mean_n_eff_clean,\n",
        "        \"mean_p_hat_wm\": mean_p_hat_wm,\n",
        "        \"mean_p_hat_clean\": mean_p_hat_clean,\n",
        "        \"delta_hat\": delta_hat,\n",
        "        \"heuristic_predicted_separation\": heuristic_predicted_separation,\n",
        "        \"empirical_separation\": empirical_separation,\n",
        "        \"fpr\": fpr.tolist(),\n",
        "        \"tpr\": tpr.tolist(),\n",
        "        \"thresholds\": thresholds.tolist(),\n",
        "        **metadata_fields,  # Include metadata fields\n",
        "    })\n",
        "\n",
        "# Create summary DataFrame\n",
        "summary_df = pd.DataFrame(summary_data)\n",
        "print(\"Summary Statistics:\")\n",
        "print(summary_df.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Ranking Configurations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Rank by primary metric: TPR @ FPR = 0.01 (or empirical separation as secondary)\n",
        "summary_df_ranked = summary_df.sort_values(\"tpr_at_1pct_fpr\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "print(\"\\nRanking by TPR @ FPR = 0.01 (Primary Metric):\")\n",
        "print(\"=\" * 80)\n",
        "for idx, row in summary_df_ranked.iterrows():\n",
        "    print(f\"{idx+1:2d}. {row['config_name']:40s} | TPR@1%: {row['tpr_at_1pct_fpr']:.4f} | AUC: {row['auc']:.4f} | N_eff_wm: {row['mean_n_eff_wm']:.1f}\")\n",
        "\n",
        "# Display full summary table with metadata and separated stats\n",
        "print(\"\\n\\nFull Summary Table:\")\n",
        "print(\"=\" * 80)\n",
        "display_cols = [\n",
        "    \"config_name\",\n",
        "    \"num_samples\",\n",
        "    \"num_inversion_steps\",\n",
        "    \"likelihood_model_path\",\n",
        "    \"tpr_at_1pct_fpr\",\n",
        "    \"auc\",\n",
        "    \"empirical_separation\",\n",
        "    \"mean_n_eff_wm\",\n",
        "    \"mean_n_eff_clean\",\n",
        "    \"mean_p_hat_wm\",\n",
        "    \"mean_p_hat_clean\",\n",
        "    \"delta_hat\",\n",
        "]\n",
        "print(summary_df_ranked[display_cols].to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Visualizations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use same path resolution as results_dir\n",
        "plots_dir = project_root / \"experiments\" / \"watermark_ablation\" / \"plots\"\n",
        "if not plots_dir.exists():\n",
        "    plots_dir = Path(\"../experiments/watermark_ablation/plots\")\n",
        "plots_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Plot 1: ROC Curves for all configs\n",
        "plt.figure(figsize=(10, 8))\n",
        "for config_name, data in results.items():\n",
        "    log_odds_wm = np.array(data[\"log_odds_wm\"])\n",
        "    log_odds_clean = np.array(data[\"log_odds_clean\"])\n",
        "    \n",
        "    # Data integrity check (should already be validated, but double-check for safety)\n",
        "    assert len(log_odds_wm) > 0 and len(log_odds_clean) > 0, (\n",
        "        f\"Empty arrays for {config_name}: wm={len(log_odds_wm)}, clean={len(log_odds_clean)}\"\n",
        "    )\n",
        "    \n",
        "    scores = np.concatenate([log_odds_wm, log_odds_clean])\n",
        "    labels = np.concatenate([np.ones(len(log_odds_wm)), np.zeros(len(log_odds_clean))])\n",
        "    \n",
        "    fpr, tpr, _ = roc_curve(labels, scores)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    \n",
        "    plt.plot(fpr, tpr, label=f\"{config_name} (AUC={roc_auc:.3f})\", linewidth=2)\n",
        "\n",
        "plt.plot([0, 1], [0, 1], \"k--\", linewidth=1, label=\"Random\")\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel(\"False Positive Rate\", fontsize=12)\n",
        "plt.ylabel(\"True Positive Rate\", fontsize=12)\n",
        "plt.title(\"ROC Curves - All Configurations\", fontsize=14)\n",
        "plt.legend(loc=\"lower right\", fontsize=8, ncol=2)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig(plots_dir / \"roc_curves_all.png\", dpi=300, bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "print(f\"‚úì Saved: {plots_dir / 'roc_curves_all.png'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot 2: Histogram overlays for each config\n",
        "for config_name, data in results.items():\n",
        "    log_odds_wm = np.array(data[\"log_odds_wm\"])\n",
        "    log_odds_clean = np.array(data[\"log_odds_clean\"])\n",
        "    \n",
        "    plt.figure(figsize=(10, 6))\n",
        "    \n",
        "    # Histogram overlay\n",
        "    plt.hist(log_odds_clean, bins=50, alpha=0.6, label=\"Clean\", color=\"blue\", density=True)\n",
        "    plt.hist(log_odds_wm, bins=50, alpha=0.6, label=\"Watermarked\", color=\"red\", density=True)\n",
        "    \n",
        "    plt.xlabel(\"Log-Odds\", fontsize=12)\n",
        "    plt.ylabel(\"Density\", fontsize=12)\n",
        "    plt.title(f\"Log-Odds Distribution: {config_name}\", fontsize=14)\n",
        "    plt.legend(fontsize=10)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    \n",
        "    # Save plot\n",
        "    plot_filename = f\"histogram_{config_name}.png\"\n",
        "    plt.savefig(plots_dir / plot_filename, dpi=300, bbox_inches=\"tight\")\n",
        "    plt.close()\n",
        "    print(f\"‚úì Saved: {plots_dir / plot_filename}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot 3: TPR @ 1% FPR comparison\n",
        "summary_df_ranked = summary_df.sort_values(\"tpr_at_1pct_fpr\", ascending=True)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "y_pos = np.arange(len(summary_df_ranked))\n",
        "plt.barh(y_pos, summary_df_ranked[\"tpr_at_1pct_fpr\"], alpha=0.7)\n",
        "plt.yticks(y_pos, summary_df_ranked[\"config_name\"])\n",
        "plt.xlabel(\"TPR @ FPR = 0.01\", fontsize=12)\n",
        "plt.title(\"TPR @ 1% FPR Comparison (Primary Ranking Metric)\", fontsize=14)\n",
        "plt.grid(True, alpha=0.3, axis=\"x\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(plots_dir / \"tpr_comparison.png\", dpi=300, bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "print(f\"‚úì Saved: {plots_dir / 'tpr_comparison.png'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot 4: N_eff_wm vs TPR @ 1% FPR\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(summary_df[\"mean_n_eff_wm\"], summary_df[\"tpr_at_1pct_fpr\"], s=100, alpha=0.6, label=\"Watermarked\")\n",
        "plt.scatter(summary_df[\"mean_n_eff_clean\"], summary_df[\"tpr_at_1pct_fpr\"], s=100, alpha=0.6, marker=\"^\", label=\"Clean\")\n",
        "for idx, row in summary_df.iterrows():\n",
        "    plt.annotate(row[\"config_name\"], (row[\"mean_n_eff_wm\"], row[\"tpr_at_1pct_fpr\"]), \n",
        "                fontsize=8, alpha=0.7)\n",
        "plt.xlabel(\"Mean N_eff\", fontsize=12)\n",
        "plt.ylabel(\"TPR @ FPR = 0.01\", fontsize=12)\n",
        "plt.title(\"N_eff vs TPR @ 1% FPR\", fontsize=14)\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig(plots_dir / \"n_eff_vs_tpr.png\", dpi=300, bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "print(f\"‚úì Saved: {plots_dir / 'n_eff_vs_tpr.png'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot 5: Heuristic Predicted vs Empirical Separation (Exploratory Only)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(summary_df[\"heuristic_predicted_separation\"], summary_df[\"empirical_separation\"], s=100, alpha=0.6)\n",
        "for idx, row in summary_df.iterrows():\n",
        "    plt.annotate(row[\"config_name\"], (row[\"heuristic_predicted_separation\"], row[\"empirical_separation\"]), \n",
        "                fontsize=8, alpha=0.7)\n",
        "\n",
        "# Add diagonal line\n",
        "min_val = min(summary_df[\"heuristic_predicted_separation\"].min(), summary_df[\"empirical_separation\"].min())\n",
        "max_val = max(summary_df[\"heuristic_predicted_separation\"].max(), summary_df[\"empirical_separation\"].max())\n",
        "plt.plot([min_val, max_val], [min_val, max_val], \"k--\", alpha=0.5, label=\"y=x\")\n",
        "\n",
        "plt.xlabel(\"Heuristic Predicted Separation (approximate: ŒîŒº ‚âà 4 * N_eff * Œ¥_hat¬≤)\", fontsize=12)\n",
        "plt.ylabel(\"Empirical Separation (mean log_odds_wm - mean log_odds_clean)\", fontsize=12)\n",
        "plt.title(\"Heuristic Predicted vs Empirical Separation (Approximate - Exploratory Only)\", fontsize=14)\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig(plots_dir / \"predicted_vs_empirical_separation.png\", dpi=300, bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "print(f\"‚úì Saved: {plots_dir / 'predicted_vs_empirical_separation.png'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üî• Fix 4: Diagnostic Visualizations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üìä Plot A ‚Äî N_eff Distributions\n",
        "# Histogram/KDE overlay for N_eff_wm and N_eff_clean per config\n",
        "for config_name, data in results.items():\n",
        "    n_eff_wm = np.array(data[\"N_eff_wm\"])\n",
        "    n_eff_clean = np.array(data[\"N_eff_clean\"])\n",
        "    \n",
        "    plt.figure(figsize=(10, 6))\n",
        "    \n",
        "    # Histogram overlay\n",
        "    plt.hist(n_eff_clean, bins=30, alpha=0.6, label=\"Clean\", color=\"blue\", density=True)\n",
        "    plt.hist(n_eff_wm, bins=30, alpha=0.6, label=\"Watermarked\", color=\"red\", density=True)\n",
        "    \n",
        "    plt.xlabel(\"N_eff\", fontsize=12)\n",
        "    plt.ylabel(\"Density\", fontsize=12)\n",
        "    plt.title(f\"N_eff Distribution: {config_name}\", fontsize=14)\n",
        "    plt.legend(fontsize=10)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    \n",
        "    plot_filename = f\"n_eff_distribution_{config_name}.png\"\n",
        "    plt.savefig(plots_dir / plot_filename, dpi=300, bbox_inches=\"tight\")\n",
        "    plt.close()\n",
        "    print(f\"‚úì Saved: {plots_dir / plot_filename}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üìä Plot B ‚Äî p_hat Distributions\n",
        "# Histogram/KDE overlay for p_hat_wm and p_hat_clean per config\n",
        "for config_name, data in results.items():\n",
        "    p_hat_wm = np.array(data[\"p_hat_wm\"])\n",
        "    p_hat_clean = np.array(data[\"p_hat_clean\"])\n",
        "    \n",
        "    plt.figure(figsize=(10, 6))\n",
        "    \n",
        "    # Histogram overlay\n",
        "    plt.hist(p_hat_clean, bins=30, alpha=0.6, label=\"Clean\", color=\"blue\", density=True)\n",
        "    plt.hist(p_hat_wm, bins=30, alpha=0.6, label=\"Watermarked\", color=\"red\", density=True)\n",
        "    \n",
        "    # Add vertical line at 0.5 (expected for clean)\n",
        "    plt.axvline(x=0.5, color=\"gray\", linestyle=\"--\", alpha=0.5, label=\"Expected (p=0.5)\")\n",
        "    \n",
        "    plt.xlabel(\"p_hat\", fontsize=12)\n",
        "    plt.ylabel(\"Density\", fontsize=12)\n",
        "    plt.title(f\"p_hat Distribution: {config_name}\", fontsize=14)\n",
        "    plt.legend(fontsize=10)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    \n",
        "    plot_filename = f\"p_hat_distribution_{config_name}.png\"\n",
        "    plt.savefig(plots_dir / plot_filename, dpi=300, bbox_inches=\"tight\")\n",
        "    plt.close()\n",
        "    print(f\"‚úì Saved: {plots_dir / plot_filename}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üìä Plot C ‚Äî Signal Geometry Scatter\n",
        "# Scatter plot: x = N_eff, y = log_odds\n",
        "# Separate for watermarked and clean to detect mask collapse, weak watermark regimes, instability\n",
        "for config_name, data in results.items():\n",
        "    log_odds_wm = np.array(data[\"log_odds_wm\"])\n",
        "    log_odds_clean = np.array(data[\"log_odds_clean\"])\n",
        "    n_eff_wm = np.array(data[\"N_eff_wm\"])\n",
        "    n_eff_clean = np.array(data[\"N_eff_clean\"])\n",
        "    \n",
        "    plt.figure(figsize=(10, 6))\n",
        "    \n",
        "    # Scatter plots\n",
        "    plt.scatter(n_eff_clean, log_odds_clean, alpha=0.6, s=50, label=\"Clean\", color=\"blue\", marker=\"o\")\n",
        "    plt.scatter(n_eff_wm, log_odds_wm, alpha=0.6, s=50, label=\"Watermarked\", color=\"red\", marker=\"s\")\n",
        "    \n",
        "    plt.xlabel(\"N_eff\", fontsize=12)\n",
        "    plt.ylabel(\"Log-Odds\", fontsize=12)\n",
        "    plt.title(f\"Signal Geometry: {config_name}\\n(Helps detect mask collapse, weak watermark regimes, instability)\", fontsize=14)\n",
        "    plt.legend(fontsize=10)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    \n",
        "    plot_filename = f\"signal_geometry_{config_name}.png\"\n",
        "    plt.savefig(plots_dir / plot_filename, dpi=300, bbox_inches=\"tight\")\n",
        "    plt.close()\n",
        "    print(f\"‚úì Saved: {plots_dir / plot_filename}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Optional: QQ Plot for Gaussianity Check\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# QQ plot for clean distribution (should be approximately Gaussian)\n",
        "# NOTE: Normality assumption may not hold for small N_eff or binary mapping\n",
        "for config_name, data in results.items():\n",
        "    log_odds_clean = np.array(data[\"log_odds_clean\"])\n",
        "    \n",
        "    # Standardize\n",
        "    mean_clean = np.mean(log_odds_clean)\n",
        "    std_clean = np.std(log_odds_clean)\n",
        "    standardized = (log_odds_clean - mean_clean) / std_clean if std_clean > 0 else log_odds_clean\n",
        "    \n",
        "    # QQ plot\n",
        "    stats.probplot(standardized, dist=\"norm\", plot=plt)\n",
        "    plt.title(\n",
        "        f\"QQ Plot (Clean Distribution): {config_name}\\n\"\n",
        "        \"(Note: Normality assumption may not hold for small N_eff or binary mapping)\",\n",
        "        fontsize=14\n",
        "    )\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    \n",
        "    plot_filename = f\"qq_plot_{config_name}.png\"\n",
        "    plt.savefig(plots_dir / plot_filename, dpi=300, bbox_inches=\"tight\")\n",
        "    plt.close()\n",
        "    print(f\"‚úì Saved: {plots_dir / plot_filename}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Save Summary Table\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save summary table to CSV\n",
        "summary_csv_path = plots_dir.parent / \"summary_table.csv\"\n",
        "summary_df_ranked.to_csv(summary_csv_path, index=False)\n",
        "print(f\"‚úì Saved summary table: {summary_csv_path}\")\n",
        "\n",
        "# Also save as JSON\n",
        "summary_json_path = plots_dir.parent / \"summary_table.json\"\n",
        "summary_df_ranked.to_json(summary_json_path, indent=2, orient=\"records\")\n",
        "print(f\"‚úì Saved summary table: {summary_json_path}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
